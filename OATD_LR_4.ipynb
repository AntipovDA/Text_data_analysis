{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Лабораторная работа №4***\n",
        "# \"Использование нейронных сетей для генерации текста\"\n",
        "# <i>Выполнил: Антипов Д.А. А-01-19</i>\n",
        "# <i>Вариант 1</i>"
      ],
      "metadata": {
        "id": "6LF3hxUgkB4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Цель работы:**"
      ],
      "metadata": {
        "id": "tyNCbt42kCps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "получить практические навыки генерации текста."
      ],
      "metadata": {
        "id": "jqsEv9QEkFKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Выполнение**"
      ],
      "metadata": {
        "id": "sn_sg9ACkJOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Загрузить выборку стихотворений одного из поэтов в соответсвии с вариантом (нечетные - Маяковский).**"
      ],
      "metadata": {
        "id": "V83aFeSvkLoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection  import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "pX7mUb-mkNt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_jsoXNIkZ1s",
        "outputId": "2230acdd-c8e9-4240-bf69-0ca777058cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Выбираем поэта\n",
        "poet = 'mayakovskiy'\n",
        "\n",
        "path_to_file = f'{poet}.txt'\n",
        "path_to_file = tf.keras.utils.get_file(path_to_file, f'http://uit.mpei.ru/git/main/TDA/raw/branch/master/assets/poems/{path_to_file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PekaDbVakcf8",
        "outputId": "75f14134-707e-41b7-9ee8-aa3561d8618f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://uit.mpei.ru/git/main/TDA/raw/branch/master/assets/poems/mayakovskiy.txt\n",
            "1466243/1466243 [==============================] - 2s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Познакомиться с данными. Проанализировать статистические характеристики исходных данных(среднюю длину стихотворения, среднюю длину строки)**"
      ],
      "metadata": {
        "id": "0cRRvTkTkhUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем текст из файла.\n",
        "# Стихотворения в файле разделены токеном '</s>' - сохраняем в переменную\n",
        "with open(path_to_file,encoding = \"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "\n",
        "EOS_TOKEN = '</s>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apMAu1rnkjUv",
        "outputId": "7d6b2632-fd19-462f-e131-1478bb8d427b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 815675 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Посмотрим на текст\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq4YDoGQkmrr",
        "outputId": "85eface4-038a-43f1-ab6e-a87550ba42f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Угрюмый дождь скосил глаза.\n",
            "А за\n",
            "решеткой\n",
            "четкой\n",
            "железной мысли проводов —\n",
            "перина.\n",
            "И на\n",
            "нее\n",
            "встающих звезд\n",
            "легко оперлись ноги.\n",
            "Но ги —\n",
            "бель фонарей,\n",
            "царей\n",
            "в короне газа,\n",
            "для глаза\n",
            "сделала больней\n",
            "враждующий букет бульварных проституток.\n",
            "И жуток\n",
            "шуток\n",
            "клюющий смех —\n",
            "из желтых\n",
            "ядовитых роз\n",
            "возрос\n",
            "зигзагом.\n",
            "За гам\n",
            "и жуть\n",
            "взглянуть\n",
            "отрадно глазу:\n",
            "раба\n",
            "крестов\n",
            "страдающе-спокойно-безразличных,\n",
            "гроба\n",
            "домов\n",
            "публичных\n",
            "восток бросал в одну пылающую вазу.\n",
            "\n",
            "</s>\n",
            "\n",
            "\n",
            "У —\n",
            "лица.\n",
            "Лица\n",
            "у\n",
            "догов\n",
            "годов\n",
            "рез —\n",
            "че.\n",
            "Че\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Подсчет статистик\n",
        "\n",
        "def mean_line_len(poem):\n",
        "    lines = [len(line.strip()) for line in poem.split('\\n') if len(line.strip())>0]\n",
        "    return round(sum(lines)/len(lines))\n",
        "\n",
        "\n",
        "def describe_poems(text,return_df = False):\n",
        "    poems_list = [poem.strip() for poem in text.split(EOS_TOKEN) if len(poem.strip())>0]\n",
        "    df = pd.DataFrame(data=poems_list,columns=['poem'])\n",
        "    df['len'] = df.poem.map(len)\n",
        "    df['lines'] = df.poem.str.count('\\n')\n",
        "    df['mean_line_len'] = df.poem.map(mean_line_len)\n",
        "    if return_df:\n",
        "        return df\n",
        "    return df.describe()"
      ],
      "metadata": {
        "id": "w0l-AWSqkqD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poem_df = describe_poems(text,return_df = True)\n",
        "poem_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "wOIefOaqktMs",
        "outputId": "375dd800-0b3b-4d9f-8fe7-1e9bd956d997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  poem   len  lines  \\\n",
              "0    Угрюмый дождь скосил глаза.\\nА за\\nрешеткой\\nч...   449     34   \n",
              "1    У —\\nлица.\\nЛица\\nу\\nдогов\\nгодов\\nрез —\\nче.\\...   546     42   \n",
              "2    «Какая очаровательная ночь!»\\n«Эта,\\n(указывае...   333     21   \n",
              "3    Скрипка издергалась, упрашивая,\\nи вдруг разре...   765     45   \n",
              "4    Войне ли думать:\\n«Некрасиво в шраме»?\\nЕй ли ...   901     69   \n",
              "..                                                 ...   ...    ...   \n",
              "738  Зеленые листики —\\nи нет зимы.\\nИдем\\nраздолье...   279     24   \n",
              "739  У меня растут года,\\nбудет и семнадцать.\\nГде ...  3656    280   \n",
              "740  За море синеволное,\\nза сто земель\\nи вод\\nраз...   666     49   \n",
              "741  Уважаемые\\nтоварищи потомки!\\nРоясь\\nв сегодня...  3681    243   \n",
              "742  Открывай страницу-дверь\\nв книжке\\nсамый разны...  1235    102   \n",
              "\n",
              "     mean_line_len  \n",
              "0               12  \n",
              "1               12  \n",
              "2               14  \n",
              "3               16  \n",
              "4               14  \n",
              "..             ...  \n",
              "738             10  \n",
              "739             13  \n",
              "740             12  \n",
              "741             14  \n",
              "742             11  \n",
              "\n",
              "[743 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4bcbef3-27ef-43a4-891b-103b545dc925\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poem</th>\n",
              "      <th>len</th>\n",
              "      <th>lines</th>\n",
              "      <th>mean_line_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Угрюмый дождь скосил глаза.\\nА за\\nрешеткой\\nч...</td>\n",
              "      <td>449</td>\n",
              "      <td>34</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>У —\\nлица.\\nЛица\\nу\\nдогов\\nгодов\\nрез —\\nче.\\...</td>\n",
              "      <td>546</td>\n",
              "      <td>42</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>«Какая очаровательная ночь!»\\n«Эта,\\n(указывае...</td>\n",
              "      <td>333</td>\n",
              "      <td>21</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Скрипка издергалась, упрашивая,\\nи вдруг разре...</td>\n",
              "      <td>765</td>\n",
              "      <td>45</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Войне ли думать:\\n«Некрасиво в шраме»?\\nЕй ли ...</td>\n",
              "      <td>901</td>\n",
              "      <td>69</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>Зеленые листики —\\nи нет зимы.\\nИдем\\nраздолье...</td>\n",
              "      <td>279</td>\n",
              "      <td>24</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>У меня растут года,\\nбудет и семнадцать.\\nГде ...</td>\n",
              "      <td>3656</td>\n",
              "      <td>280</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>За море синеволное,\\nза сто земель\\nи вод\\nраз...</td>\n",
              "      <td>666</td>\n",
              "      <td>49</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>Уважаемые\\nтоварищи потомки!\\nРоясь\\nв сегодня...</td>\n",
              "      <td>3681</td>\n",
              "      <td>243</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>Открывай страницу-дверь\\nв книжке\\nсамый разны...</td>\n",
              "      <td>1235</td>\n",
              "      <td>102</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>743 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4bcbef3-27ef-43a4-891b-103b545dc925')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4bcbef3-27ef-43a4-891b-103b545dc925 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4bcbef3-27ef-43a4-891b-103b545dc925');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poem_df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "pinN7z2LkveU",
        "outputId": "9b6b1bc2-a175-4e56-a320-75a346167eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               len       lines  mean_line_len\n",
              "count   743.000000  743.000000     743.000000\n",
              "mean   1088.702557   81.532974      12.900404\n",
              "std     623.199169   49.312455       2.407488\n",
              "min     203.000000   19.000000       8.000000\n",
              "25%     632.000000   44.000000      11.000000\n",
              "50%     952.000000   72.000000      12.000000\n",
              "75%    1345.500000  106.000000      15.000000\n",
              "max    4172.000000  287.000000      18.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35ca6bc5-be37-4754-a3ac-fb9a52a1cfc7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>lines</th>\n",
              "      <th>mean_line_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>743.000000</td>\n",
              "      <td>743.000000</td>\n",
              "      <td>743.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1088.702557</td>\n",
              "      <td>81.532974</td>\n",
              "      <td>12.900404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>623.199169</td>\n",
              "      <td>49.312455</td>\n",
              "      <td>2.407488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>203.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>632.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>11.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>952.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1345.500000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4172.000000</td>\n",
              "      <td>287.000000</td>\n",
              "      <td>18.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35ca6bc5-be37-4754-a3ac-fb9a52a1cfc7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35ca6bc5-be37-4754-a3ac-fb9a52a1cfc7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35ca6bc5-be37-4754-a3ac-fb9a52a1cfc7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Подготовить выборку для обучения**"
      ],
      "metadata": {
        "id": "qezb-tKsk3x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Разбиваем данные на тренировочные, валидационные и тестовые\n",
        "\n",
        "train_poems, test_poems = train_test_split(poem_df.poem.to_list(),test_size = 0.1, random_state = 42)\n",
        "train_poems, val_poems = train_test_split(train_poems,test_size = 0.1, random_state = 42)\n",
        "\n",
        "train_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(train_poems)\n",
        "val_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(val_poems)\n",
        "test_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(test_poems)"
      ],
      "metadata": {
        "id": "3lnvmqggk4Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Словарь уникальных символов из текста.\n",
        "\n",
        "vocab = sorted(set(text))+[EOS_TOKEN]\n",
        "print(f'{len(vocab)} unique characters')\n",
        "print (vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Jv5wuFk7LB",
        "outputId": "2eb30a80-032c-404f-9f98-686167da7f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140 unique characters\n",
            "['\\n', ' ', '!', '\"', '%', '&', '(', ')', ',', '-', '.', '/', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'J', 'K', 'M', 'N', 'O', 'P', 'R', 'S', 'U', 'V', 'X', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'z', '\\xa0', '«', '»', 'à', 'ç', 'è', 'ö', 'ü', '̀', '́', '·', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', '–', '—', '’', '…', '№', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Кодируем текст в виде числовой последовательности для подачи на вход нейронной сети\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy().decode('utf-8')\n",
        "\n",
        "def ids_from_text(text):\n",
        "    return ids_from_chars(tf.strings.unicode_split(text, input_encoding='UTF-8'))"
      ],
      "metadata": {
        "id": "QPAuOk28lJjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример кодирования\n",
        "ids = ids_from_text(train_poems[:20])\n",
        "res_text = text_from_ids(ids)\n",
        "print(train_poems[:20],ids,res_text,sep = '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioYZ2SsHlaOK",
        "outputId": "190d8c8f-5187-44b4-9df0-9b3e516566ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "У Петровой\n",
            "у Надежды\n",
            "tf.Tensor(\n",
            "[ 90   2  86 107 120 118 116 104 116 111   1 121   2  84 102 106 107 108\n",
            " 106 129], shape=(20,), dtype=int64)\n",
            "У Петровой\n",
            "у Надежды\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Кодируем данные и преобразуем их в Датасеты\n",
        "train_ids = ids_from_text(train_poems)\n",
        "val_ids = ids_from_text(val_poems)\n",
        "test_ids = ids_from_text(test_poems)\n",
        "\n",
        "train_ids_dataset = tf.data.Dataset.from_tensor_slices(train_ids)\n",
        "val_ids_dataset = tf.data.Dataset.from_tensor_slices(val_ids)\n",
        "test_ids_dataset = tf.data.Dataset.from_tensor_slices(test_ids)"
      ],
      "metadata": {
        "id": "AX9PsV43lhhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(train_ids_dataset)//(seq_length+1)"
      ],
      "metadata": {
        "id": "kuG7iiCrllJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = train_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "val_sequences = val_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "test_sequences = test_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in train_sequences.take(1):\n",
        "  print(text_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxWHf26ll_9v",
        "outputId": "6aba558f-89e6-4d23-fefc-dccd70778a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "У Петровой\n",
            "у Надежды\n",
            "не имеется одежды.\n",
            "Чтоб купить\n",
            "(пришли деньки!),\n",
            "не имеется деньги́.\n",
            "Ей\n",
            "в расцве\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем датасет с input и target строками\n",
        "# target сдвинута относительно input на один символ.\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "3Y33ztWemGQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Маяковский\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAbvzeLvmNSf",
        "outputId": "961d5979-2939-40db-bc9e-9fdfcf6d4ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['М', 'а', 'я', 'к', 'о', 'в', 'с', 'к', 'и'],\n",
              " ['а', 'я', 'к', 'о', 'в', 'с', 'к', 'и', 'й'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_sequences.map(split_input_target)\n",
        "val_dataset = val_sequences.map(split_input_target)\n",
        "test_dataset = test_sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "52hZa_q0mY72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in val_dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example))\n",
        "    print(\"Target:\", text_from_ids(target_example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFinLEBUmfrW",
        "outputId": "861066b9-07ef-4536-a2d8-8f4cb275dc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : В смокинг вштопорен,\n",
            "побрит что надо.\n",
            "По гранд\n",
            "по опере\n",
            "гуляю грандом.\n",
            "Смотрю\n",
            "в антракте —\n",
            "красавка \n",
            "Target:  смокинг вштопорен,\n",
            "побрит что надо.\n",
            "По гранд\n",
            "по опере\n",
            "гуляю грандом.\n",
            "Смотрю\n",
            "в антракте —\n",
            "красавка н\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перемешиваем датасеты и разбиваем их на батчи для оптимизации обучения\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    dataset = (\n",
        "        dataset\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "    return dataset\n",
        "\n",
        "train_dataset = prepare_dataset(train_dataset)\n",
        "val_dataset = prepare_dataset(val_dataset)\n",
        "test_dataset = prepare_dataset(test_dataset)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg3n-8qCmnHq",
        "outputId": "ea386739-61be-44d4-f5b2-cc04ff22cbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Построить нейронную сеть. Тип ячейки RNN выбрать в соответсвии с вариантом (LSTM)**"
      ],
      "metadata": {
        "id": "iPMuZ5MenSAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Длина словаря символов\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# размерность Embedding'а\n",
        "embedding_dim = 256 #@param{type:\"number\"}\n",
        "\n",
        "# Параметры RNN-слоя\n",
        "rnn_units = 300 #@param {type:\"number\"}\n",
        "dropout_p = 0.5"
      ],
      "metadata": {
        "id": "RmqiExl9nSsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                   dropout = dropout_p,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    if states is None:\n",
        "        states = self.lstm.get_initial_state(x)\n",
        "\n",
        "    x, *states = self.lstm(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "xDsLFx84ng8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5 = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "_nLWFou-nlpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на один батч из датасета\n",
        "for input_example_batch, target_example_batch in train_dataset.take(1):\n",
        "    example_batch_predictions = model_5(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbVT5AKtnvay",
        "outputId": "20b3af05-c493-431c-94ec-4d39c4a75eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 141) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DejSxTcxpL3Q",
        "outputId": "c459b363-ac71-455e-dddd-e2c84a8264de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(141,), dtype=float32, numpy=\n",
              "array([ 1.6046757e-03,  2.5796483e-03, -1.5955156e-03, -6.6421466e-04,\n",
              "       -4.8838546e-03,  4.7933506e-03,  1.9701705e-03,  4.8641930e-03,\n",
              "        7.6674633e-03, -5.9319953e-03, -3.7517908e-03, -1.5375147e-03,\n",
              "       -1.7565720e-03, -9.0143160e-04, -4.3582306e-03, -6.3046939e-03,\n",
              "        1.9063733e-03,  6.7528249e-03, -4.3004849e-03, -2.6088553e-03,\n",
              "       -1.4683431e-03,  1.8297128e-03,  9.6582025e-03, -4.2914250e-03,\n",
              "       -7.4972410e-04,  6.4173471e-03,  4.6165832e-03,  1.9842413e-05,\n",
              "        2.8695904e-03,  1.9777992e-03, -1.6086627e-03, -8.7593524e-03,\n",
              "        1.6836994e-03,  4.7581512e-03, -4.5777418e-04, -3.9604022e-03,\n",
              "       -4.3548364e-03, -6.9737206e-03, -5.1254506e-04, -5.8917870e-04,\n",
              "       -3.4650380e-03,  1.6049956e-03, -7.3621566e-03, -6.0576452e-03,\n",
              "        5.5510816e-03, -4.9042841e-03,  7.0305308e-04,  6.3169107e-04,\n",
              "       -3.9747049e-04, -4.0708221e-03,  1.2660704e-02, -9.4204834e-03,\n",
              "        1.2156089e-02,  9.5698482e-04,  7.9990993e-04,  4.5075333e-03,\n",
              "        7.7825980e-03,  4.5117736e-03, -1.8501369e-03, -7.9275190e-04,\n",
              "       -8.2147680e-03,  3.8615559e-04,  2.0682337e-03, -3.1978970e-03,\n",
              "        5.6536864e-03,  1.6987807e-03,  1.1535365e-03,  3.9821453e-03,\n",
              "        4.0988261e-03,  5.8047981e-03, -8.0392268e-03, -3.7899148e-03,\n",
              "        8.6589186e-03, -9.4720144e-03,  5.4142335e-03,  8.0860443e-03,\n",
              "        1.2173039e-03,  1.1759114e-03, -4.4746497e-03,  5.8980164e-04,\n",
              "       -1.7501592e-03,  5.7255090e-03,  6.6325376e-03, -3.5823052e-04,\n",
              "        2.5357802e-03, -3.7534900e-03, -4.2880792e-03,  1.2550084e-03,\n",
              "        2.3709990e-03,  7.4730832e-03, -2.0355801e-03,  5.7111913e-04,\n",
              "        6.6338279e-03,  5.8868984e-03,  2.5341322e-03, -1.1005765e-03,\n",
              "       -3.1170493e-03,  4.1768230e-03,  7.8704982e-04,  4.8400968e-04,\n",
              "       -4.2301142e-03, -7.8016282e-03,  3.3917979e-03,  2.0518841e-03,\n",
              "        4.0916976e-04, -5.2161110e-03, -7.9906145e-03,  1.9892778e-03,\n",
              "       -7.1297092e-03,  1.3634989e-03,  8.4140821e-04, -3.1160654e-03,\n",
              "        3.0671165e-03, -4.8391274e-03, -2.6355099e-03, -4.2076875e-03,\n",
              "        1.2029790e-03, -1.9213179e-03,  1.4378841e-02,  9.3814237e-03,\n",
              "        3.6369101e-03, -7.7317806e-04, -6.5049888e-03,  1.7091841e-03,\n",
              "       -4.3429788e-03, -2.4554599e-03,  8.4909389e-04,  8.5468413e-03,\n",
              "        7.5794756e-03, -4.1061635e-03, -3.7603932e-03,  2.4637093e-03,\n",
              "        6.2673003e-03,  2.2854336e-04,  4.2182351e-03, -3.6185144e-03,\n",
              "        8.4584969e-04, -1.7996805e-03,  9.3054527e-04,  4.0208778e-04,\n",
              "       -3.7314731e-04], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r6PemGYpOQd",
        "outputId": "d85ea203-fe50-4313-e540-f18e2aafedd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([110,  40,   4, 125, 100,   4,  67, 125, 109,  34,   8, 126,  79,\n",
              "       119, 102, 135,  56,   7,  82,  62,  89,  24, 125,  90,  72, 129,\n",
              "        14,  70,  75,  20,  91,  80,  83,  79,  51,  24, 138,  67,  75,\n",
              "       130, 134,   7,  65, 129, 115,   1, 101, 136, 123, 130,  20, 107,\n",
              "        37, 140,  69,  75,  41,   3, 110, 119, 106,  45,  32,  77, 124,\n",
              "        35, 111,  30,  39, 128,  50,   3, 115, 123,  40,  65, 112,  12,\n",
              "       103,  86,  88,  58, 102, 137,   4,  55, 105, 132, 101,  84, 117,\n",
              "         3,  60,  85,  35,  57,  83,  89, 101, 118])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1plFEN2LpmSM",
        "outputId": "b76c2eaf-c33e-47e2-f9ef-f2b44a6781c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " ть.\n",
            "Эй,\n",
            "проснитесь, которые спят!\n",
            "Разоблачай\n",
            "с головы до пят.\n",
            "Товарищ,\n",
            "не смей молчать!\n",
            "\n",
            "</s>\n",
            "\n",
            "Много\n",
            "\n",
            "Next Char Predictions:\n",
            " иc\"чЮ\"üчзU)шИса–u(Л»ТHчУБы;·ДCФЙМИoH…üДьё(èын\n",
            "Я—хьCеZ</s>́Дd!исдhRЖцVйObъn!нхcèк/бПСwа’\"tгюЯНп! ОVvМТЯр\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Обучить нейронную сеть на разных количествах эпох (5, 15, 30, 50, 70) при зафиксированных параметрах embedding_dim = 256, rnn_units = 300, T = 0.3 и сравнить результаты генерации (тексты), перплексию и статистические характеристики сгенерированных текстов. Выбрать оптимальное количество эпох**"
      ],
      "metadata": {
        "id": "S2ovEgh_przB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "O0r3Q8Mip2xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-uz9LohqHD7",
        "outputId": "eefe38ff-a838-4c00-a649-42ce8d249638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 141)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.948912, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('perplexity: ',np.exp(example_batch_mean_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynbj-r2RqTfY",
        "outputId": "f0f1da0c-95d9-42ad-e591-73816640ac32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  141.02147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.compile(optimizer='adam', loss=loss)\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "nUSVY9_K3f6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e42b731-cac2-4cd5-98fe-35230e83fb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  36096     \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  668400    \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  42441     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 746,937\n",
            "Trainable params: 746,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    monitor=\"val_loss\",\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "mSvO1DhOqhGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5 = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "model_5.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "7Vm8IaSlqpsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS_5 = 5\n",
        "EPOCHS_15 = 15\n",
        "EPOCHS_30 = 30\n",
        "EPOCHS_50 = 50\n",
        "EPOCHS_70 = 70"
      ],
      "metadata": {
        "id": "B8FHNKN6rRc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 5 эпохах\n",
        "history_5 = model_5.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_5, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBtFr2dHrYi3",
        "outputId": "af1225b5-3b68-4744-b1b6-4c973e246e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "102/102 [==============================] - 8s 51ms/step - loss: 3.4207 - val_loss: 2.9701\n",
            "Epoch 2/5\n",
            "102/102 [==============================] - 4s 25ms/step - loss: 2.8098 - val_loss: 2.6927\n",
            "Epoch 3/5\n",
            "102/102 [==============================] - 3s 21ms/step - loss: 2.6550 - val_loss: 2.6008\n",
            "Epoch 4/5\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 2.5869 - val_loss: 2.5438\n",
            "Epoch 5/5\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.5384 - val_loss: 2.5041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_5.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g43q7qoM3ng",
        "outputId": "15577bad-1716-4e7a-d6f7-4a5b5d76e3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 12ms/step - loss: 2.4997\n",
            "eval loss: 2.499728202819824\n",
            "perplexity 12.179183243139676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сбросим веса модели перед очередным обучением\n",
        "model_5 = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "model_5.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "iYUKuH3AT60T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 15 эпохах\n",
        "history_15 = model_5.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_15, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIQW4b7sNC7e",
        "outputId": "8a6988be-4369-43ff-8889-38aeffe6f131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "102/102 [==============================] - 9s 45ms/step - loss: 3.4186 - val_loss: 2.9397\n",
            "Epoch 2/15\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.7847 - val_loss: 2.6883\n",
            "Epoch 3/15\n",
            "102/102 [==============================] - 4s 26ms/step - loss: 2.6535 - val_loss: 2.6012\n",
            "Epoch 4/15\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.5899 - val_loss: 2.5508\n",
            "Epoch 5/15\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.5447 - val_loss: 2.5118\n",
            "Epoch 6/15\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.5081 - val_loss: 2.4778\n",
            "Epoch 7/15\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.4777 - val_loss: 2.4507\n",
            "Epoch 8/15\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4508 - val_loss: 2.4255\n",
            "Epoch 9/15\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4270 - val_loss: 2.4046\n",
            "Epoch 10/15\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.4056 - val_loss: 2.3852\n",
            "Epoch 11/15\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.3854 - val_loss: 2.3654\n",
            "Epoch 12/15\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3674 - val_loss: 2.3484\n",
            "Epoch 13/15\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3503 - val_loss: 2.3283\n",
            "Epoch 14/15\n",
            "102/102 [==============================] - 4s 20ms/step - loss: 2.3332 - val_loss: 2.3157\n",
            "Epoch 15/15\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3173 - val_loss: 2.3004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_5.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r273c76hPIPX",
        "outputId": "8ee56a01-224a-4478-d609-18b9b82d4990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 12ms/step - loss: 2.2907\n",
            "eval loss: 2.2907278537750244\n",
            "perplexity 9.88212780820568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сбросим веса модели перед очередным обучением\n",
        "model_5 = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "model_5.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "Ji49sB7MUEmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 30 эпохах\n",
        "history_30 = model_5.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_30, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLpSmCkFPOVs",
        "outputId": "4a93b197-9bd5-4d60-b9ec-c0d08e344664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "102/102 [==============================] - 8s 45ms/step - loss: 3.4119 - val_loss: 2.9427\n",
            "Epoch 2/30\n",
            "102/102 [==============================] - 5s 27ms/step - loss: 2.7842 - val_loss: 2.6802\n",
            "Epoch 3/30\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.6511 - val_loss: 2.5992\n",
            "Epoch 4/30\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.5853 - val_loss: 2.5406\n",
            "Epoch 5/30\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.5369 - val_loss: 2.5024\n",
            "Epoch 6/30\n",
            "102/102 [==============================] - 4s 22ms/step - loss: 2.5004 - val_loss: 2.4722\n",
            "Epoch 7/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4711 - val_loss: 2.4462\n",
            "Epoch 8/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4465 - val_loss: 2.4233\n",
            "Epoch 9/30\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.4243 - val_loss: 2.4027\n",
            "Epoch 10/30\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.4039 - val_loss: 2.3835\n",
            "Epoch 11/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3850 - val_loss: 2.3671\n",
            "Epoch 12/30\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.3689 - val_loss: 2.3507\n",
            "Epoch 13/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3520 - val_loss: 2.3354\n",
            "Epoch 14/30\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.3362 - val_loss: 2.3194\n",
            "Epoch 15/30\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.3214 - val_loss: 2.3029\n",
            "Epoch 16/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3069 - val_loss: 2.2925\n",
            "Epoch 17/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2937 - val_loss: 2.2792\n",
            "Epoch 18/30\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2805 - val_loss: 2.2641\n",
            "Epoch 19/30\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.2676 - val_loss: 2.2510\n",
            "Epoch 20/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2551 - val_loss: 2.2377\n",
            "Epoch 21/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2429 - val_loss: 2.2253\n",
            "Epoch 22/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2312 - val_loss: 2.2160\n",
            "Epoch 23/30\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2203 - val_loss: 2.2042\n",
            "Epoch 24/30\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2091 - val_loss: 2.1968\n",
            "Epoch 25/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1988 - val_loss: 2.1854\n",
            "Epoch 26/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1892 - val_loss: 2.1758\n",
            "Epoch 27/30\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.1787 - val_loss: 2.1667\n",
            "Epoch 28/30\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1696 - val_loss: 2.1568\n",
            "Epoch 29/30\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.1600 - val_loss: 2.1510\n",
            "Epoch 30/30\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.1518 - val_loss: 2.1403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_5.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYq1q_5tPzuX",
        "outputId": "fb8ebf93-c90b-4d6c-eb95-f2be0c17fdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 12ms/step - loss: 2.1438\n",
            "eval loss: 2.1438262462615967\n",
            "perplexity 8.532020866332582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сбросим веса модели перед очередным обучением\n",
        "model_5 = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "model_5.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "fG84NS2WUFyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 50 эпохах\n",
        "history_50 = model_5.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voB4kSUxP6SZ",
        "outputId": "b66c7a13-478e-4127-9c83-22d3241c4696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "102/102 [==============================] - 9s 51ms/step - loss: 3.4080 - val_loss: 2.9444\n",
            "Epoch 2/50\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.7769 - val_loss: 2.6765\n",
            "Epoch 3/50\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 2.6453 - val_loss: 2.5911\n",
            "Epoch 4/50\n",
            "102/102 [==============================] - 4s 22ms/step - loss: 2.5765 - val_loss: 2.5333\n",
            "Epoch 5/50\n",
            "102/102 [==============================] - 3s 21ms/step - loss: 2.5267 - val_loss: 2.4957\n",
            "Epoch 6/50\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 2.4904 - val_loss: 2.4582\n",
            "Epoch 7/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4617 - val_loss: 2.4358\n",
            "Epoch 8/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.4361 - val_loss: 2.4113\n",
            "Epoch 9/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4128 - val_loss: 2.3920\n",
            "Epoch 10/50\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 2.3931 - val_loss: 2.3716\n",
            "Epoch 11/50\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.3745 - val_loss: 2.3548\n",
            "Epoch 12/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.3570 - val_loss: 2.3359\n",
            "Epoch 13/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3402 - val_loss: 2.3184\n",
            "Epoch 14/50\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.3247 - val_loss: 2.3052\n",
            "Epoch 15/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.3102 - val_loss: 2.2919\n",
            "Epoch 16/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2960 - val_loss: 2.2768\n",
            "Epoch 17/50\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 2.2827 - val_loss: 2.2601\n",
            "Epoch 18/50\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.2694 - val_loss: 2.2500\n",
            "Epoch 19/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2563 - val_loss: 2.2373\n",
            "Epoch 20/50\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.2447 - val_loss: 2.2274\n",
            "Epoch 21/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2337 - val_loss: 2.2146\n",
            "Epoch 22/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2235 - val_loss: 2.2033\n",
            "Epoch 23/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2133 - val_loss: 2.1924\n",
            "Epoch 24/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2021 - val_loss: 2.1871\n",
            "Epoch 25/50\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.1919 - val_loss: 2.1773\n",
            "Epoch 26/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1836 - val_loss: 2.1680\n",
            "Epoch 27/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1746 - val_loss: 2.1604\n",
            "Epoch 28/50\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.1652 - val_loss: 2.1511\n",
            "Epoch 29/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1569 - val_loss: 2.1439\n",
            "Epoch 30/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1481 - val_loss: 2.1373\n",
            "Epoch 31/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1412 - val_loss: 2.1299\n",
            "Epoch 32/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.1338 - val_loss: 2.1236\n",
            "Epoch 33/50\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.1265 - val_loss: 2.1162\n",
            "Epoch 34/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1194 - val_loss: 2.1121\n",
            "Epoch 35/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1114 - val_loss: 2.1044\n",
            "Epoch 36/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.1057 - val_loss: 2.0988\n",
            "Epoch 37/50\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.0992 - val_loss: 2.0954\n",
            "Epoch 38/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0928 - val_loss: 2.0912\n",
            "Epoch 39/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0865 - val_loss: 2.0845\n",
            "Epoch 40/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0807 - val_loss: 2.0822\n",
            "Epoch 41/50\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.0746 - val_loss: 2.0762\n",
            "Epoch 42/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0688 - val_loss: 2.0715\n",
            "Epoch 43/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0641 - val_loss: 2.0677\n",
            "Epoch 44/50\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 2.0580 - val_loss: 2.0621\n",
            "Epoch 45/50\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.0524 - val_loss: 2.0581\n",
            "Epoch 46/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0476 - val_loss: 2.0558\n",
            "Epoch 47/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0424 - val_loss: 2.0520\n",
            "Epoch 48/50\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0369 - val_loss: 2.0465\n",
            "Epoch 49/50\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.0325 - val_loss: 2.0431\n",
            "Epoch 50/50\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0275 - val_loss: 2.0384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_5.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni5nbsn5Q_Vs",
        "outputId": "f0d03ace-8b91-4070-c638-5b8212be7b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 11ms/step - loss: 2.0499\n",
            "eval loss: 2.049909830093384\n",
            "perplexity 7.767200706967358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сбросим веса модели перед очередным обучением\n",
        "model_5 = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "model_5.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "pIZq7v8bUGo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "history_70 = model_5.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58oXsweTRER6",
        "outputId": "1a16ed03-87e8-4bc9-968e-b9abebd95948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 9s 50ms/step - loss: 3.4129 - val_loss: 2.9523\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 2.7891 - val_loss: 2.6851\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.6549 - val_loss: 2.6047\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.5939 - val_loss: 2.5559\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 3s 21ms/step - loss: 2.5511 - val_loss: 2.5159\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.5165 - val_loss: 2.4865\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.4883 - val_loss: 2.4617\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.4636 - val_loss: 2.4397\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4423 - val_loss: 2.4202\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.4218 - val_loss: 2.3988\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.4022 - val_loss: 2.3801\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 2.3834 - val_loss: 2.3632\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3667 - val_loss: 2.3442\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.3497 - val_loss: 2.3280\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3337 - val_loss: 2.3130\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.3195 - val_loss: 2.3000\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.3046 - val_loss: 2.2848\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2915 - val_loss: 2.2716\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2785 - val_loss: 2.2582\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.2659 - val_loss: 2.2451\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.2536 - val_loss: 2.2331\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2419 - val_loss: 2.2226\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.2306 - val_loss: 2.2107\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.2191 - val_loss: 2.2005\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.2085 - val_loss: 2.1890\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1977 - val_loss: 2.1820\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 4s 20ms/step - loss: 2.1885 - val_loss: 2.1714\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.1787 - val_loss: 2.1634\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1697 - val_loss: 2.1529\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.1614 - val_loss: 2.1454\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.1514 - val_loss: 2.1410\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.1444 - val_loss: 2.1314\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 2.1353 - val_loss: 2.1244\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1275 - val_loss: 2.1197\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.1192 - val_loss: 2.1118\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.1132 - val_loss: 2.1053\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.1057 - val_loss: 2.0976\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0979 - val_loss: 2.0914\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0906 - val_loss: 2.0841\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0841 - val_loss: 2.0811\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 4s 20ms/step - loss: 2.0771 - val_loss: 2.0759\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0718 - val_loss: 2.0681\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0643 - val_loss: 2.0675\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.0593 - val_loss: 2.0598\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0527 - val_loss: 2.0596\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0482 - val_loss: 2.0496\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.0413 - val_loss: 2.0461\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 2.0369 - val_loss: 2.0449\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0316 - val_loss: 2.0395\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0256 - val_loss: 2.0367\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 2.0202 - val_loss: 2.0336\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0161 - val_loss: 2.0284\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0110 - val_loss: 2.0270\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.0057 - val_loss: 2.0245\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.0017 - val_loss: 2.0191\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 1.9976 - val_loss: 2.0141\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9926 - val_loss: 2.0141\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9882 - val_loss: 2.0101\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 4s 20ms/step - loss: 1.9841 - val_loss: 2.0096\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9794 - val_loss: 2.0065\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9752 - val_loss: 2.0021\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9721 - val_loss: 2.0001\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 1.9681 - val_loss: 1.9994\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 4s 18ms/step - loss: 1.9641 - val_loss: 1.9988\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 1.9606 - val_loss: 1.9923\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9560 - val_loss: 1.9901\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 4s 20ms/step - loss: 1.9535 - val_loss: 1.9900\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9498 - val_loss: 1.9876\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 3s 18ms/step - loss: 1.9472 - val_loss: 1.9852\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 1.9432 - val_loss: 1.9834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_5.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abjmc1T0SQF2",
        "outputId": "6defa3c8-028f-43f0-e938-69cc3781a2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 11ms/step - loss: 1.9966\n",
            "eval loss: 1.9966121912002563\n",
            "perplexity 7.364065744829364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Результат**"
      ],
      "metadata": {
        "id": "nkkSek2hSyjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| epochs | eval_loss | perplexity |\n",
        "|--------|-----------|------------|\n",
        "| 5      | 2.500     | 12.179     |\n",
        "| 15     | 2.291     | 9.882      |\n",
        "| 30     | 2.144     | 8.532      |\n",
        "| 50     | 2.050     | 7.767      |\n",
        "| 70     | 1.997     | 7.364      |"
      ],
      "metadata": {
        "id": "k3PgOOCDS29p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Судя по полученным результатам, оптимальное количество эпох соответствует 70 (epochs=70)."
      ],
      "metadata": {
        "id": "YLj8RE-UUNO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Изменяя параметр температуры T проанализировать изменения сгенерированного текста. Выбрать оптимальное значение параметра.**"
      ],
      "metadata": {
        "id": "fUEgjIp5Ut5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем модель реализующую один шаг предсказания:\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "\n",
        "  # Этот фрагмент целиком написан с использованием Tensorflow, поэтому его можно выполнять\n",
        "  # не с помощью интерпретатора языка Python, а через граф операций. Это будет значительно быстрее.\n",
        "  # Для этого воспользуемся декоратором  @tf.function\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None,temperature=1.0):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "ytGd1hquU7im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_5, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "paH2upGSVxI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 0.5 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5GLR0BFV6L3",
        "outputId": "519af125-7702-47c3-f3bb-12cb4ca15a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "и тебе\n",
            "на только от держать в полед.\n",
            "Гордо странами\n",
            "не стоит под нам —\n",
            "еще\n",
            "не стоял на весь.\n",
            "Мать бы —\n",
            "развестите,\n",
            "в копейках стройка,\n",
            "в плеча\n",
            "и в теле\n",
            "на старом лотки.\n",
            "Объедительство\n",
            "в работать\n",
            "в помня\n",
            "старая торого.\n",
            "Не верней восторга —\n",
            "вы развернется\n",
            "в ответ\n",
            "за никой\n",
            "с небесной строе последний.\n",
            "И в сорок в колебе\n",
            "солнце молодежения в пользуйте.\n",
            "Должно оскально —\n",
            "другой\n",
            "под союзется надо —\n",
            "чем полне под коммунизм.\n",
            "Идите,\n",
            "что же одна —\n",
            "так свое согласенье —\n",
            "против Иваности на требует по раскатал.\n",
            "Ответственный\n",
            "на слева день,\n",
            "от завод\n",
            "слова на полнула в хлебов.\n",
            "Любовь\n",
            "на примерит победину\n",
            "все внимательной руких и другом.\n",
            "\n",
            "</s>\n",
            "\n",
            "О выходят миллионам\n",
            "стальной картон.\n",
            "\n",
            "</s>\n",
            "\n",
            "Не стройте полеса,\n",
            "что домом и кого я.\n",
            "И верст всех делего —\n",
            "как жизнь\n",
            "показываются дело.\n",
            "\n",
            "</s>\n",
            "\n",
            "В как без разгода —\n",
            "деловест протретив,\n",
            "оконец и время,\n",
            "обороненье к строга —\n",
            "в оконок,\n",
            "потымать по радостной,\n",
            "а мерением выгод —\n",
            "конец без страна просто.\n",
            "На земле\n",
            "не за комсомольцами ваши!\n",
            "\n",
            "</s>\n",
            "\n",
            "Страна не в коммуна картан\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2805771827697754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "describe_poems(result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "_4y7Zi0-WRZn",
        "outputId": "3464af58-201b-4d9c-e31a-3d5bba4619b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              len      lines  mean_line_len\n",
              "count    5.000000   5.000000            5.0\n",
              "mean   193.600000  10.200000           19.0\n",
              "std    251.293255  15.385058            4.0\n",
              "min     26.000000   0.000000           16.0\n",
              "25%     36.000000   1.000000           17.0\n",
              "50%     89.000000   4.000000           18.0\n",
              "75%    189.000000   9.000000           18.0\n",
              "max    628.000000  37.000000           26.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61b3f0e7-5803-4f97-90cf-e928d8c2c543\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>lines</th>\n",
              "      <th>mean_line_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>193.600000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>251.293255</td>\n",
              "      <td>15.385058</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>36.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>89.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>189.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>628.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61b3f0e7-5803-4f97-90cf-e928d8c2c543')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61b3f0e7-5803-4f97-90cf-e928d8c2c543 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61b3f0e7-5803-4f97-90cf-e928d8c2c543');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Проанализировать зависимость перплексии, скорости обучения, результатов генерации от параметров нейронной сети embedding_dim, rnn_units: embedding_dim = {vocab/4, vocab/2, vocab, vocab * 2, vocab * 4}, где vocab = размер словаря выборки. rnn_units = {10, 100, 300, 500}**"
      ],
      "metadata": {
        "id": "xxHMZ2YgcqIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# размерность Embedding'а\n",
        "embedding_dim = [vocab_size//4, vocab_size//2, vocab_size, vocab_size*2, vocab_size*4]\n",
        "\n",
        "# Параметры RNN-слоя\n",
        "rnn_units = [10, 100, 300, 500]"
      ],
      "metadata": {
        "id": "P05cG_K9cuIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size//4\n",
        "# rnn_units = 10\n",
        "\n",
        "model_var_embed = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[0],\n",
        "    rnn_units=rnn_units[0])\n",
        "model_var_embed.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "h-DE8ey3d5R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "history_70 = model_var_embed.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_8uiYlUfxO4",
        "outputId": "c2681300-65f3-4eca-e003-4ddffdfbdd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 10s 63ms/step - loss: 4.4208 - val_loss: 3.7913\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 3.6034 - val_loss: 3.5154\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.4855 - val_loss: 3.4655\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 3.4481 - val_loss: 3.4338\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 3s 16ms/step - loss: 3.4246 - val_loss: 3.4160\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 3.4064 - val_loss: 3.3937\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.3805 - val_loss: 3.3589\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.3374 - val_loss: 3.3046\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.2775 - val_loss: 3.2437\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 3.2237 - val_loss: 3.1970\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.1793 - val_loss: 3.1545\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.1424 - val_loss: 3.1199\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.1117 - val_loss: 3.0911\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.0849 - val_loss: 3.0664\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.0611 - val_loss: 3.0437\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 3.0394 - val_loss: 3.0239\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 3.0203 - val_loss: 3.0069\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.0032 - val_loss: 2.9907\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.9871 - val_loss: 2.9745\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.9677 - val_loss: 2.9439\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.9368 - val_loss: 2.9191\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.9133 - val_loss: 2.8925\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8940 - val_loss: 2.8763\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8789 - val_loss: 2.8628\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.8656 - val_loss: 2.8499\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 3s 14ms/step - loss: 2.8548 - val_loss: 2.8391\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.8444 - val_loss: 2.8298\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8348 - val_loss: 2.8201\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8260 - val_loss: 2.8117\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8171 - val_loss: 2.8018\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8072 - val_loss: 2.7919\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 3s 14ms/step - loss: 2.7967 - val_loss: 2.7809\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.7875 - val_loss: 2.7707\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7781 - val_loss: 2.7601\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7695 - val_loss: 2.7536\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7615 - val_loss: 2.7465\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.7546 - val_loss: 2.7405\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.7480 - val_loss: 2.7333\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7415 - val_loss: 2.7269\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7354 - val_loss: 2.7214\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.7295 - val_loss: 2.7162\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7244 - val_loss: 2.7105\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.7192 - val_loss: 2.7063\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.7148 - val_loss: 2.7008\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7102 - val_loss: 2.6975\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7062 - val_loss: 2.6916\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7019 - val_loss: 2.6893\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6984 - val_loss: 2.6852\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.6947 - val_loss: 2.6823\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6913 - val_loss: 2.6790\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6883 - val_loss: 2.6758\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6851 - val_loss: 2.6726\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6824 - val_loss: 2.6698\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 3s 14ms/step - loss: 2.6795 - val_loss: 2.6688\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.6776 - val_loss: 2.6655\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6752 - val_loss: 2.6642\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6731 - val_loss: 2.6613\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6711 - val_loss: 2.6591\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6687 - val_loss: 2.6573\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6669 - val_loss: 2.6554\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6653 - val_loss: 2.6547\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6633 - val_loss: 2.6525\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6617 - val_loss: 2.6515\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6603 - val_loss: 2.6489\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6589 - val_loss: 2.6480\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6575 - val_loss: 2.6466\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6554 - val_loss: 2.6447\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6541 - val_loss: 2.6420\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6533 - val_loss: 2.6414\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.6515 - val_loss: 2.6389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_embed.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WkmgF1tgOWC",
        "outputId": "41af04e4-732c-4b7e-fb75-b1b0d88c2d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 5ms/step - loss: 2.6303\n",
            "eval loss: 2.630324363708496\n",
            "perplexity 13.878270779507359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_embed, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.8\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNpsVR2Sga07",
        "outputId": "0a1da54b-c296-47ff-f48e-98b943108099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ушимь мать —\n",
            "бодковы.\n",
            "Выминты,\n",
            "преб — восёй обри.\n",
            "Слам мацят ято\n",
            "бов грибуй,\n",
            "моеко.\n",
            "Вымалдровюраристуши?\n",
            "Не —\n",
            "шыме.\n",
            "Т Втачеки собитен,\n",
            "в раро.\n",
            "Лаз вота вала\n",
            "с вщалей сесщара,\n",
            "а пол и иб корси\n",
            "узратузилиши,\n",
            "зежареро вогов,\n",
            "гапусекой,\n",
            "вом дофета зотель опотий\n",
            "ни поры\n",
            "дулно\n",
            "сюхь про сгуни —\n",
            "сет,\n",
            "в зестянича прурирак чедо старет,\n",
            "на мам.\n",
            "Пупод\n",
            "вуз с несьмеки меливась раездавол,\n",
            "габротом диси стега\n",
            "пор прол зилевиана\n",
            "пролых\n",
            "вочки\n",
            "скароз,\n",
            "вел ни те те крестись\n",
            "сох тае зла,\n",
            "тему себева наой нувиты\n",
            "и оте.\n",
            "Ра к бодое.\n",
            "Чомолщи на в сшееня,\n",
            "и —\n",
            "пола во —\n",
            "в ригом нас,\n",
            "—\n",
            "е канарики рев гореложях в мазакна\n",
            "наеми шедос\n",
            "одут\n",
            "гроргу\n",
            "при\n",
            "столе\n",
            "вался.\n",
            "Нествой\n",
            "недой —\n",
            "зае отознева тый дем звоюду».\n",
            "Врит жвылы вопито.\n",
            "Валин идасте потосн даннебь\n",
            "гонет\n",
            "«Скевасиких кынах стол —\n",
            "рами огелеранесавна\n",
            "вметол,\n",
            "ла брочат баплить првем бомет сле невнивый бол птоль?\n",
            "Евраци ред в кочель вомточесты тоши моколи пожтак.\n",
            "с на дазкыт тый\n",
            "ны чдиста\n",
            "стобци етлает.\n",
            "Зае нав гориче ствее,\n",
            "потякь.\n",
            "Сзласкожик.\n",
            "Несь нав вомдать не\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.907393455505371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size//2\n",
        "# rnn_units = 10\n",
        "\n",
        "model_var_embed = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[1],\n",
        "    rnn_units=rnn_units[0])\n",
        "model_var_embed.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "z4wAxVZ7hMwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "history_70 = model_var_embed.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2iPhziphle9",
        "outputId": "665a745e-b92f-43e3-8598-d5bf9439abe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 9s 54ms/step - loss: 4.4006 - val_loss: 3.8024\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 2s 14ms/step - loss: 3.6085 - val_loss: 3.5138\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 2s 13ms/step - loss: 3.4812 - val_loss: 3.4586\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 3.4410 - val_loss: 3.4310\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 3.4188 - val_loss: 3.4079\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.3939 - val_loss: 3.3814\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 3.3652 - val_loss: 3.3467\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.3255 - val_loss: 3.3006\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.2747 - val_loss: 3.2483\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 3.2237 - val_loss: 3.1990\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 3s 13ms/step - loss: 3.1796 - val_loss: 3.1603\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.1446 - val_loss: 3.1302\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.1166 - val_loss: 3.1000\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.0846 - val_loss: 3.0639\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.0437 - val_loss: 3.0216\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 3.0065 - val_loss: 2.9878\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 2.9759 - val_loss: 2.9606\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.9507 - val_loss: 2.9355\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.9277 - val_loss: 2.9151\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.9054 - val_loss: 2.8904\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.8828 - val_loss: 2.8721\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.8642 - val_loss: 2.8532\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.8470 - val_loss: 2.8355\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8275 - val_loss: 2.8153\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8092 - val_loss: 2.7981\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7928 - val_loss: 2.7817\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7784 - val_loss: 2.7686\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7658 - val_loss: 2.7558\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.7542 - val_loss: 2.7455\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7434 - val_loss: 2.7345\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7343 - val_loss: 2.7263\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7262 - val_loss: 2.7180\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7184 - val_loss: 2.7106\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 2.7117 - val_loss: 2.7048\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7050 - val_loss: 2.6995\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7000 - val_loss: 2.6936\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6954 - val_loss: 2.6890\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6901 - val_loss: 2.6833\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.6858 - val_loss: 2.6792\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 2.6814 - val_loss: 2.6744\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6769 - val_loss: 2.6699\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6733 - val_loss: 2.6670\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6695 - val_loss: 2.6617\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6657 - val_loss: 2.6582\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6620 - val_loss: 2.6542\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.6589 - val_loss: 2.6504\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6557 - val_loss: 2.6470\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6524 - val_loss: 2.6436\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6495 - val_loss: 2.6416\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6470 - val_loss: 2.6375\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6443 - val_loss: 2.6353\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6421 - val_loss: 2.6329\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6394 - val_loss: 2.6303\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6369 - val_loss: 2.6279\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6354 - val_loss: 2.6262\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6330 - val_loss: 2.6241\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.6315 - val_loss: 2.6214\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6292 - val_loss: 2.6204\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6278 - val_loss: 2.6186\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6254 - val_loss: 2.6177\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.6242 - val_loss: 2.6154\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.6227 - val_loss: 2.6131\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6213 - val_loss: 2.6124\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6199 - val_loss: 2.6108\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6183 - val_loss: 2.6096\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6168 - val_loss: 2.6074\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.6153 - val_loss: 2.6059\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.6144 - val_loss: 2.6047\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6123 - val_loss: 2.6020\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6114 - val_loss: 2.6018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_embed.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_buJcFjhYF7",
        "outputId": "904c7ad4-b1fd-4853-cfc7-3a778be4bd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 5ms/step - loss: 2.5933\n",
            "eval loss: 2.593261480331421\n",
            "perplexity 13.373317364323043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_embed, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.5\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RySNWvKYhaHt",
        "outputId": "f600c17e-8a79-4466-9a0c-6c43185c977b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "пруште.\n",
            "Дани —\n",
            "в мераки со трони короебов толитей столони в сторет порни\n",
            "наз подае повом сась невнини лони в брабось сторики,\n",
            "налотей —\n",
            "в гот ке ладо встастить\n",
            "в кор —\n",
            "в номе орет\n",
            "голиму.\n",
            "Что прадь вавазак\n",
            "че трозать.\n",
            "Зарась пет ворикить полеговня зариветь ма празого тулаки\n",
            "бодать,\n",
            "на в «Порять\n",
            "нем ом пор —\n",
            "к от стостинаванков разако,\n",
            "бовом, —\n",
            "сторь короте\n",
            "в кориль гредаю в порем замом до в волистой\n",
            "сно корем в на\n",
            "помего\n",
            "с борамни на под\n",
            "делим\n",
            "доторитыт\n",
            "слот,\n",
            "ни сторнух навет\n",
            "сто воволь тет и в милуть,\n",
            "в —\n",
            "долонись —\n",
            "заривет порах вобелистет,\n",
            "в реболо\n",
            "не ватовиней.\n",
            "Одим, пробеннам\n",
            "им нона помуще подий тростой скок позет че гостей\n",
            "сто с в —\n",
            "на на полекинатов\n",
            "бесто поразнет\n",
            "в нера в столь ос нин ното нергой колоно\n",
            "подит.\n",
            "\n",
            "\n",
            "С стого селегол под разды,\n",
            "де важи в валь борето восту од тами.\n",
            "Кориши пучий вростлит,\n",
            "с —\n",
            "полить поратой пок —\n",
            "в продем —\n",
            "содо кразась рыд нами вынок сторожа вореля бо наневы\n",
            "на польсит,\n",
            "на ма порабы с пиди,\n",
            "в о раз к не на в полиник порет,\n",
            "слосто воде в дерся на пора\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.874347686767578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size\n",
        "# rnn_units = 10\n",
        "\n",
        "model_var_embed = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[2],\n",
        "    rnn_units=rnn_units[0])\n",
        "model_var_embed.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "dIZ97vyTifF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "history_70 = model_var_embed.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7r-6JlCiqiX",
        "outputId": "781b0a22-25c4-41fa-df61-02bdb97258b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 8s 48ms/step - loss: 4.3561 - val_loss: 3.7424\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 4s 19ms/step - loss: 3.5872 - val_loss: 3.5095\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 3.4755 - val_loss: 3.4530\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 3.4385 - val_loss: 3.4291\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 3.4148 - val_loss: 3.4034\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.3861 - val_loss: 3.3689\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 3.3432 - val_loss: 3.3150\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 3.2816 - val_loss: 3.2492\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.2210 - val_loss: 3.1957\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.1713 - val_loss: 3.1467\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 3.1263 - val_loss: 3.1059\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.0907 - val_loss: 3.0739\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 2s 13ms/step - loss: 3.0623 - val_loss: 3.0491\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 3.0386 - val_loss: 3.0267\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 3.0172 - val_loss: 3.0051\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.9962 - val_loss: 2.9851\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.9752 - val_loss: 2.9598\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.9464 - val_loss: 2.9286\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.9157 - val_loss: 2.9037\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.8939 - val_loss: 2.8834\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8720 - val_loss: 2.8579\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8482 - val_loss: 2.8383\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.8291 - val_loss: 2.8206\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.8118 - val_loss: 2.8026\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7953 - val_loss: 2.7871\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7793 - val_loss: 2.7699\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7639 - val_loss: 2.7561\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7506 - val_loss: 2.7432\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.7382 - val_loss: 2.7313\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7262 - val_loss: 2.7186\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7157 - val_loss: 2.7089\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.7050 - val_loss: 2.6973\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.6959 - val_loss: 2.6882\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 2.6876 - val_loss: 2.6803\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6808 - val_loss: 2.6732\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6739 - val_loss: 2.6658\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6685 - val_loss: 2.6603\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6629 - val_loss: 2.6540\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6582 - val_loss: 2.6486\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.6537 - val_loss: 2.6455\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6501 - val_loss: 2.6416\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6462 - val_loss: 2.6380\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6423 - val_loss: 2.6345\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6390 - val_loss: 2.6312\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6359 - val_loss: 2.6285\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.6327 - val_loss: 2.6251\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.6294 - val_loss: 2.6218\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6264 - val_loss: 2.6182\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6239 - val_loss: 2.6158\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6213 - val_loss: 2.6129\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6185 - val_loss: 2.6106\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6155 - val_loss: 2.6085\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6133 - val_loss: 2.6056\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6108 - val_loss: 2.6028\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6086 - val_loss: 2.6014\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6068 - val_loss: 2.5995\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6048 - val_loss: 2.5968\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6030 - val_loss: 2.5938\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6003 - val_loss: 2.5932\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5982 - val_loss: 2.5902\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5961 - val_loss: 2.5882\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5943 - val_loss: 2.5867\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.5925 - val_loss: 2.5838\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.5907 - val_loss: 2.5828\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5892 - val_loss: 2.5806\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5873 - val_loss: 2.5800\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5856 - val_loss: 2.5781\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5842 - val_loss: 2.5765\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.5821 - val_loss: 2.5751\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5812 - val_loss: 2.5733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_embed.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3N_NI0tisgd",
        "outputId": "4348c9f8-6591-450b-ce5d-644bb2533eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 5ms/step - loss: 2.5635\n",
            "eval loss: 2.5635249614715576\n",
            "perplexity 12.9814960337465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_embed, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.5\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL-J1LsxiyF8",
        "outputId": "05a1a4c6-1e73-4722-da83-c057face2566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Суторе в троду,\n",
            "в обоми.\n",
            "Не да свастяль бо не в ворака на в в сторовонь под растолься.\n",
            "Не не дарев\n",
            "стопра\n",
            "сторовов ит гаривай валь\n",
            "масторо в лез ит\n",
            "о дота по трастем стодара стороть, строваров старанов посеть дени\n",
            "несколь\n",
            "б\n",
            "на настой в не лись роскора на беровованит погуры в кране на вать сторовов не недро в торся сидем\n",
            "и не горостно сулох и тоты скуротом.\n",
            "На машелий это гроны всто тут гольной ворат\n",
            "и порана\n",
            "на ваторить тасть спорьния\n",
            "в в стоновить зеговатной ботель в разегова\n",
            "полох\n",
            "я стотоли\n",
            "процеколит вобероручся сдесноднай пок в прозогудорне то доголове\n",
            "лаши сорамне,\n",
            "в овери поливетрини\n",
            "за крастом\n",
            "за до недовини в дидям\n",
            "и горожя\n",
            "неба не дель,\n",
            "пате и столи мерять мельноть потульно —\n",
            "поласта стордый калькитной омены\n",
            "короди итель перобо малим в пороном челься вод вобылька\n",
            "мостить родя стоно траско\n",
            "в кораста,\n",
            "дористи в лече толь о о манит стодит\n",
            "до просовы\n",
            "до дуль неда\n",
            "датле,\n",
            "бомома,\n",
            "на далот не дото бырия комы —\n",
            "раздя палом овотом бероми котам полештей беланов не сторя в хогона,\n",
            "полить\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5357861518859863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size*2\n",
        "# rnn_units = 10\n",
        "\n",
        "model_var_embed = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[3],\n",
        "    rnn_units=rnn_units[0])\n",
        "model_var_embed.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "laBkbv1XkYWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "history_70 = model_var_embed.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGYG3BywkkcA",
        "outputId": "9f3c7910-c0b1-42c2-ca53-cd108c0df174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 8s 51ms/step - loss: 4.3157 - val_loss: 3.7009\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 3s 21ms/step - loss: 3.5671 - val_loss: 3.4978\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 3s 16ms/step - loss: 3.4667 - val_loss: 3.4495\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 3.4346 - val_loss: 3.4241\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 3.4112 - val_loss: 3.4010\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 3.3814 - val_loss: 3.3591\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.3323 - val_loss: 3.3038\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 3.2694 - val_loss: 3.2365\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 3.1995 - val_loss: 3.1632\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 3.1200 - val_loss: 3.0821\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 3.0489 - val_loss: 3.0189\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.9928 - val_loss: 2.9716\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.9528 - val_loss: 2.9378\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.9226 - val_loss: 2.9105\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8978 - val_loss: 2.8865\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.8757 - val_loss: 2.8660\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.8562 - val_loss: 2.8473\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.8400 - val_loss: 2.8321\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.8261 - val_loss: 2.8200\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.8134 - val_loss: 2.8067\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.7966 - val_loss: 2.7855\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7766 - val_loss: 2.7653\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7583 - val_loss: 2.7508\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7432 - val_loss: 2.7350\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 2.7299 - val_loss: 2.7241\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7192 - val_loss: 2.7138\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7100 - val_loss: 2.7057\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7024 - val_loss: 2.6983\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 2s 13ms/step - loss: 2.6952 - val_loss: 2.6916\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.6890 - val_loss: 2.6853\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6836 - val_loss: 2.6795\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6778 - val_loss: 2.6740\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6723 - val_loss: 2.6684\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6669 - val_loss: 2.6633\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6625 - val_loss: 2.6585\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6582 - val_loss: 2.6543\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6539 - val_loss: 2.6506\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6505 - val_loss: 2.6469\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 2.6473 - val_loss: 2.6430\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6437 - val_loss: 2.6398\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6406 - val_loss: 2.6369\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6380 - val_loss: 2.6350\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6349 - val_loss: 2.6314\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6321 - val_loss: 2.6294\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.6293 - val_loss: 2.6258\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6267 - val_loss: 2.6241\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6248 - val_loss: 2.6213\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6224 - val_loss: 2.6194\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6202 - val_loss: 2.6166\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6173 - val_loss: 2.6145\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6152 - val_loss: 2.6110\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6129 - val_loss: 2.6078\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6096 - val_loss: 2.6049\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6071 - val_loss: 2.6023\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6049 - val_loss: 2.6002\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 3s 8ms/step - loss: 2.6029 - val_loss: 2.5982\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6004 - val_loss: 2.5955\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5982 - val_loss: 2.5942\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5965 - val_loss: 2.5923\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5942 - val_loss: 2.5896\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.5923 - val_loss: 2.5871\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.5901 - val_loss: 2.5855\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5884 - val_loss: 2.5841\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5862 - val_loss: 2.5821\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.5843 - val_loss: 2.5807\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5825 - val_loss: 2.5784\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5806 - val_loss: 2.5764\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 3s 10ms/step - loss: 2.5783 - val_loss: 2.5748\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5770 - val_loss: 2.5729\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5751 - val_loss: 2.5719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_embed.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_57Wvxj_knjr",
        "outputId": "d1bfe8e0-c48f-4a60-8f19-39fb331cd241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 5ms/step - loss: 2.5635\n",
            "eval loss: 2.5635037422180176\n",
            "perplexity 12.981220579013314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_embed, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.5\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADH98EOOkoLd",
        "outputId": "1d52237c-ca9f-4476-f318-864d5076d949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "мастрити на в поде керет\n",
            "ташия\n",
            "в спорика\n",
            "голевак ток грова воро баброга.\n",
            "Паде\n",
            "уде колимашь побо — поли на зель,\n",
            "прогрова красте горерень\n",
            "карки прастодной в сторельки прастонты\n",
            "пововый ноне\n",
            "сшиной онинянь нат,\n",
            "стрико.\n",
            "Моста\n",
            "порестерное содий грасте на стольелит не —\n",
            "грабром\n",
            "пелина не лем верит,\n",
            "словели\n",
            "растодо\n",
            "прододил,\n",
            "кроберонить\n",
            "на ссетто вобной —\n",
            "молака,\n",
            "в —\n",
            "бодан\n",
            "и бодет болить дагорох\n",
            "разна спорит нель натна дерки солости\n",
            "в воскрабенить\n",
            "и поритоды\n",
            "ожарни —\n",
            "овадонит\n",
            "поликни,\n",
            "бомали спазат\n",
            "и на мы не мать\n",
            "одит —\n",
            "сомилет мазитот,\n",
            "поскоредимама сестори\n",
            "толоде,\n",
            "в воликаго.\n",
            "Стасто —\n",
            "в и сверит в настить\n",
            "кот порито стумы и воду\n",
            "в воболь и сла пруд\n",
            "и наб маркои достов\n",
            "в кросто\n",
            "и воте черну —\n",
            "и востре бору сет страт\n",
            "и скоборит банить и бом подат постоль,\n",
            "кат в и о на в солит стут нитнели\n",
            "выста разда стернох\n",
            "и полестах\n",
            "в помалать.\n",
            "\n",
            "</s>\n",
            "\n",
            "\n",
            "\n",
            "</s>\n",
            "\n",
            "Сшебен\n",
            "в растот в подей —\n",
            "в оперо доне\n",
            "не в татя настит\n",
            "и пропонь\n",
            "горену —\n",
            "больчат масти бовебе —\n",
            "не на стольты,\n",
            "одань\n",
            "на наболь лости,\n",
            "на польколи\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.9714386463165283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size*4\n",
        "# rnn_units = 10\n",
        "\n",
        "model_var_embed = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[4],\n",
        "    rnn_units=rnn_units[0])\n",
        "model_var_embed.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "Wt8l6khAlp9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "history_70 = model_var_embed.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbZ-PwZelw_N",
        "outputId": "0952ca67-b183-4535-a85d-a21ce0dd2ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 10s 57ms/step - loss: 4.2686 - val_loss: 3.7092\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 3s 20ms/step - loss: 3.5821 - val_loss: 3.5116\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 3.4758 - val_loss: 3.4547\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.4393 - val_loss: 3.4290\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 3s 15ms/step - loss: 3.4043 - val_loss: 3.3850\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 3.3637 - val_loss: 3.3409\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 3.3094 - val_loss: 3.2771\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.2491 - val_loss: 3.2249\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 3.1922 - val_loss: 3.1610\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 3.1213 - val_loss: 3.0844\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 3.0526 - val_loss: 3.0273\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 3.0027 - val_loss: 2.9836\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.9651 - val_loss: 2.9526\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.9376 - val_loss: 2.9280\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.9150 - val_loss: 2.9056\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8926 - val_loss: 2.8835\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.8738 - val_loss: 2.8659\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.8572 - val_loss: 2.8490\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.8416 - val_loss: 2.8340\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.8268 - val_loss: 2.8175\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.8097 - val_loss: 2.8015\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.7931 - val_loss: 2.7840\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.7737 - val_loss: 2.7647\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.7565 - val_loss: 2.7479\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7415 - val_loss: 2.7357\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.7284 - val_loss: 2.7217\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.7163 - val_loss: 2.7097\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.7053 - val_loss: 2.7005\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6949 - val_loss: 2.6898\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6850 - val_loss: 2.6810\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.6770 - val_loss: 2.6741\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6703 - val_loss: 2.6691\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6640 - val_loss: 2.6628\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6580 - val_loss: 2.6576\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.6521 - val_loss: 2.6508\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.6458 - val_loss: 2.6457\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6402 - val_loss: 2.6420\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6355 - val_loss: 2.6376\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6307 - val_loss: 2.6318\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.6261 - val_loss: 2.6280\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.6222 - val_loss: 2.6240\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.6187 - val_loss: 2.6204\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6155 - val_loss: 2.6175\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6125 - val_loss: 2.6149\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.6089 - val_loss: 2.6112\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.6050 - val_loss: 2.6061\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.6014 - val_loss: 2.6038\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.5985 - val_loss: 2.6006\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.5955 - val_loss: 2.5977\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5933 - val_loss: 2.5965\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5909 - val_loss: 2.5943\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.5890 - val_loss: 2.5918\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.5864 - val_loss: 2.5900\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5844 - val_loss: 2.5878\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.5822 - val_loss: 2.5849\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5800 - val_loss: 2.5832\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 2s 9ms/step - loss: 2.5783 - val_loss: 2.5815\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.5766 - val_loss: 2.5798\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5746 - val_loss: 2.5782\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 2s 8ms/step - loss: 2.5731 - val_loss: 2.5769\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5715 - val_loss: 2.5756\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.5700 - val_loss: 2.5725\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.5683 - val_loss: 2.5725\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 3s 11ms/step - loss: 2.5670 - val_loss: 2.5707\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 2s 10ms/step - loss: 2.5660 - val_loss: 2.5705\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5647 - val_loss: 2.5682\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5637 - val_loss: 2.5671\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.5626 - val_loss: 2.5654\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 3s 9ms/step - loss: 2.5615 - val_loss: 2.5658\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 2s 7ms/step - loss: 2.5604 - val_loss: 2.5644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_embed.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK_sZ-pyly8r",
        "outputId": "796fe93d-37a7-4fd8-bd3e-ec5e671fc3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 6ms/step - loss: 2.5523\n",
            "eval loss: 2.55229115486145\n",
            "perplexity 12.836480481187785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_embed, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.5\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZMBGfS5l1bD",
        "outputId": "4951655c-0643-4b81-d148-a2ed411f27c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "настой.\n",
            "Привели стозгорат выре от в столи то от памнана веревь\n",
            "исторут воннён за и порось в от спродит —\n",
            "на в просторать в либов на простотый та пора сностой в менать торь\n",
            "жак в на сторнити\n",
            "на,\n",
            "нах достой на порялонный на селит.\n",
            "Стото кодут нама гриколик востать вом на тадит в пронси кудной коралай стары —\n",
            "в на непорем\n",
            "не стовестой\n",
            "не на с крома моко валове в в бы дут столикатер\n",
            "не созеть.\n",
            "и побат порнесь корна\n",
            "сто в расевирит сто прунов возаники —\n",
            "и велие —\n",
            "са дрость моль и стохаки,\n",
            "и там\n",
            "в сторони,\n",
            "калит в доратае,\n",
            "за продит\n",
            "на вет прусесь погирес,\n",
            "на потитой отит слести\n",
            "сорерие\n",
            "сласчиви мимали —\n",
            "в количный потом коме поздес,\n",
            "кололий.\n",
            "\n",
            "Потоники\n",
            "на коря —\n",
            "доря.\n",
            "На пророденнат\n",
            "на доны —\n",
            "и настался порки рес —\n",
            "перодажать\n",
            "на престоли.\n",
            "— На дометь трато мерако —\n",
            "па слеченцел\n",
            "на незели!\n",
            "\n",
            "В и столице.\n",
            "Чтараниньь тоты рестарска нанет ногорай погок слевого поделинать,\n",
            "шта корка. —\n",
            "В доли и порить не ста стаме жет порко насто олиздо\n",
            "насет лебры поростой всенит че востай вырочи.\n",
            "Простовит блим\n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.8679416179656982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| epochs | embedding_dim | rnn_units | eval_loss | perplexity |\n",
        "|--------|---------------|-----------|-----------|------------|\n",
        "| 70     | vocab_size//4 | 10        | 2.630     | 13.878     |\n",
        "| 70     | vocab_size//2 | 10        | 2.593     | 13.373     |\n",
        "| 70     | vocab_size    | 10        | 2.563     | 12.981     |\n",
        "| 70     | vocab_size*2  | 10        | 2.563     | 12.981     |\n",
        "| 70     | vocab_size*4  | 10        | 2.552     | 12.836     |"
      ],
      "metadata": {
        "id": "pa7g7u-Hn-2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size*4\n",
        "# rnn_units = 100\n",
        "\n",
        "model_var_rnn_units = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[4],\n",
        "    rnn_units=rnn_units[1])\n",
        "model_var_rnn_units.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "kXb6l2x3pnSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "start = time.time()\n",
        "history_70 = model_var_rnn_units.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])\n",
        "end = time.time()\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDOIAZh3pthR",
        "outputId": "48fd945b-1aed-4f3c-9caa-a4ed5f137a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 7s 38ms/step - loss: 3.5635 - val_loss: 3.0763\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 3s 19ms/step - loss: 2.9062 - val_loss: 2.7934\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 3s 16ms/step - loss: 2.7344 - val_loss: 2.6846\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 3s 17ms/step - loss: 2.6581 - val_loss: 2.6264\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 3s 14ms/step - loss: 2.6120 - val_loss: 2.5878\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.5782 - val_loss: 2.5599\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.5522 - val_loss: 2.5350\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 2s 13ms/step - loss: 2.5297 - val_loss: 2.5135\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.5100 - val_loss: 2.4932\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 3s 13ms/step - loss: 2.4917 - val_loss: 2.4775\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.4753 - val_loss: 2.4611\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.4599 - val_loss: 2.4470\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.4459 - val_loss: 2.4316\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.4332 - val_loss: 2.4215\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 3s 13ms/step - loss: 2.4214 - val_loss: 2.4095\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.4098 - val_loss: 2.3988\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.3991 - val_loss: 2.3888\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.3893 - val_loss: 2.3783\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 2s 13ms/step - loss: 2.3796 - val_loss: 2.3705\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.3705 - val_loss: 2.3625\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.3620 - val_loss: 2.3531\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.3541 - val_loss: 2.3443\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.3455 - val_loss: 2.3371\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.3376 - val_loss: 2.3290\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.3290 - val_loss: 2.3226\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.3208 - val_loss: 2.3138\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.3131 - val_loss: 2.3053\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.3052 - val_loss: 2.2987\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2982 - val_loss: 2.2911\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.2914 - val_loss: 2.2849\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 3s 14ms/step - loss: 2.2853 - val_loss: 2.2796\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2791 - val_loss: 2.2724\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2730 - val_loss: 2.2671\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2675 - val_loss: 2.2622\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2622 - val_loss: 2.2588\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.2569 - val_loss: 2.2538\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.2518 - val_loss: 2.2472\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2464 - val_loss: 2.2416\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.2419 - val_loss: 2.2388\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2375 - val_loss: 2.2351\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2324 - val_loss: 2.2305\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 3s 14ms/step - loss: 2.2287 - val_loss: 2.2265\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.2239 - val_loss: 2.2218\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2198 - val_loss: 2.2170\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2162 - val_loss: 2.2143\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 2s 13ms/step - loss: 2.2121 - val_loss: 2.2100\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.2081 - val_loss: 2.2071\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2043 - val_loss: 2.2025\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.2007 - val_loss: 2.1981\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1969 - val_loss: 2.1959\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.1937 - val_loss: 2.1921\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 3s 13ms/step - loss: 2.1904 - val_loss: 2.1901\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1868 - val_loss: 2.1875\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1835 - val_loss: 2.1836\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1800 - val_loss: 2.1786\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1770 - val_loss: 2.1766\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 3s 14ms/step - loss: 2.1739 - val_loss: 2.1747\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1708 - val_loss: 2.1721\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1679 - val_loss: 2.1694\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1647 - val_loss: 2.1663\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1625 - val_loss: 2.1639\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 2s 13ms/step - loss: 2.1596 - val_loss: 2.1609\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.1570 - val_loss: 2.1585\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1543 - val_loss: 2.1574\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1517 - val_loss: 2.1539\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1491 - val_loss: 2.1527\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 2s 12ms/step - loss: 2.1457 - val_loss: 2.1510\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 3s 12ms/step - loss: 2.1440 - val_loss: 2.1470\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1412 - val_loss: 2.1456\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 2s 11ms/step - loss: 2.1395 - val_loss: 2.1435\n",
            "\n",
            "Run time: 190.0073013305664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_rnn_units.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExCptqXCp4HA",
        "outputId": "ddda4d9e-dfcd-4c02-db1e-979aa43f9bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 8ms/step - loss: 2.1456\n",
            "eval loss: 2.1456058025360107\n",
            "perplexity 8.547217595305451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_rnn_units, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.5\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ-NEjtRp9U8",
        "outputId": "caea7a8f-e072-4be3-eaa9-6a148fd819b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "</s>\n",
            "\n",
            "В богать на состит поднем,\n",
            "настретитель\n",
            "у верное\n",
            "в обеда\n",
            "под в коммунисториями —\n",
            "под самой всему\n",
            "под слова на последный будто\n",
            "не теперь —\n",
            "маличайте —\n",
            "приметали в обратились стара.\n",
            "Столько моем такие\n",
            "на теперь\n",
            "в это вот красней назовой.\n",
            "Страники в просторой не летах\n",
            "пропятный меня\n",
            "в прополица в граничись\n",
            "под стали\n",
            "меня\n",
            "в небо за просту ни от стран.\n",
            "\n",
            "</s>\n",
            "\n",
            "Рукой просло!\n",
            "На от седите не домой.\n",
            "Как под старитель\n",
            "служай в откран.\n",
            "В полобородими\n",
            "в ного\n",
            "сказал красный разверх.\n",
            "А будет с бару\n",
            "в большем вымерите,\n",
            "а корот\n",
            "запорет с простить.\n",
            "Ленин —\n",
            "по дерладище и старать на толгов.\n",
            "\n",
            "\n",
            "Все сили\n",
            "на поками\n",
            "по во в свое\n",
            "по картились\n",
            "и выстритай в хазали разворде.\n",
            "Столок заботорые самого ни вы покова.\n",
            "На старительный мородиться картиться,\n",
            "когда\n",
            "признать зачень\n",
            "под половодами.\n",
            "А под нежно —\n",
            "не раздало из коммунилось постариты.\n",
            "\n",
            "</s>\n",
            "\n",
            "Ты\n",
            "рабочий стальный странцума.\n",
            "Если всех в голово.\n",
            "Помоганцы\n",
            "и головочный поезда\n",
            "и волнце в него летерами\n",
            "на водайцами гряшей.\n",
            "Дорогой обойде с мини́\n",
            "поравестая\n",
            "в с\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.9367477893829346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size*4\n",
        "# rnn_units = 300\n",
        "\n",
        "model_var_rnn_units = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[4],\n",
        "    rnn_units=rnn_units[2])\n",
        "model_var_rnn_units.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "q1Ofycfrrdds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "start = time.time()\n",
        "history_70 = model_var_rnn_units.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])\n",
        "end = time.time()\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHZGXfThrj4b",
        "outputId": "0319d7aa-1435-4094-98e9-214a16e811af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 9s 58ms/step - loss: 3.2658 - val_loss: 2.7926\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 4s 26ms/step - loss: 2.6915 - val_loss: 2.6196\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.5860 - val_loss: 2.5441\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 4s 27ms/step - loss: 2.5255 - val_loss: 2.4919\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.4808 - val_loss: 2.4557\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 2.4468 - val_loss: 2.4249\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.4171 - val_loss: 2.3997\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 4s 22ms/step - loss: 2.3914 - val_loss: 2.3770\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.3685 - val_loss: 2.3530\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.3479 - val_loss: 2.3335\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.3271 - val_loss: 2.3136\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 2.3081 - val_loss: 2.2958\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.2906 - val_loss: 2.2792\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 3s 24ms/step - loss: 2.2736 - val_loss: 2.2623\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 2.2578 - val_loss: 2.2480\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 3s 24ms/step - loss: 2.2428 - val_loss: 2.2329\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.2281 - val_loss: 2.2178\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.2142 - val_loss: 2.2054\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.2003 - val_loss: 2.1917\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 3s 24ms/step - loss: 2.1872 - val_loss: 2.1808\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.1746 - val_loss: 2.1674\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.1624 - val_loss: 2.1595\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.1506 - val_loss: 2.1448\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.1391 - val_loss: 2.1386\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.1289 - val_loss: 2.1273\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.1180 - val_loss: 2.1177\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.1085 - val_loss: 2.1091\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.0989 - val_loss: 2.1051\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 2.0900 - val_loss: 2.0958\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.0804 - val_loss: 2.0880\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.0718 - val_loss: 2.0828\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.0639 - val_loss: 2.0761\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.0560 - val_loss: 2.0713\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.0482 - val_loss: 2.0640\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.0404 - val_loss: 2.0590\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.0335 - val_loss: 2.0523\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.0263 - val_loss: 2.0472\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 2.0199 - val_loss: 2.0446\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 2.0143 - val_loss: 2.0393\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.0068 - val_loss: 2.0345\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 2.0005 - val_loss: 2.0296\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 1.9943 - val_loss: 2.0264\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9884 - val_loss: 2.0253\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9830 - val_loss: 2.0198\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 1.9776 - val_loss: 2.0124\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9724 - val_loss: 2.0107\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 1.9670 - val_loss: 2.0071\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9619 - val_loss: 2.0042\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 1.9565 - val_loss: 2.0060\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9521 - val_loss: 2.0009\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9466 - val_loss: 1.9973\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 1.9421 - val_loss: 1.9940\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9375 - val_loss: 1.9925\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9329 - val_loss: 1.9878\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 1.9292 - val_loss: 1.9868\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 1.9250 - val_loss: 1.9869\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9209 - val_loss: 1.9833\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 1.9168 - val_loss: 1.9812\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9129 - val_loss: 1.9794\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.9092 - val_loss: 1.9779\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 4s 25ms/step - loss: 1.9050 - val_loss: 1.9745\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 4s 23ms/step - loss: 1.9014 - val_loss: 1.9723\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.8966 - val_loss: 1.9704\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 3s 24ms/step - loss: 1.8948 - val_loss: 1.9712\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 1.8904 - val_loss: 1.9665\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 3s 22ms/step - loss: 1.8867 - val_loss: 1.9677\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 1.8833 - val_loss: 1.9657\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 1.8799 - val_loss: 1.9661\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 3s 23ms/step - loss: 1.8763 - val_loss: 1.9635\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 4s 24ms/step - loss: 1.8744 - val_loss: 1.9630\n",
            "\n",
            "Run time: 288.9612441062927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_rnn_units.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWH5SDchrmdk",
        "outputId": "6c1bc609-c224-4574-abc7-c0785093046b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 13ms/step - loss: 1.9745\n",
            "eval loss: 1.9745146036148071\n",
            "perplexity 7.203122435528266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_rnn_units, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.5\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oogGroEtruU-",
        "outputId": "c267bbaa-9fb2-43b4-ef21-1b2637b0571e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "выше открытый строгов.\n",
            "\n",
            "\n",
            ". Первый\n",
            "веков и советский радость.\n",
            "Положил\n",
            "столом ворот не сразу.\n",
            "По самого кровью поля,\n",
            "в коммунисты\n",
            "в рабочий продавали в бородился —\n",
            "с любители\n",
            "клочка\n",
            "в темного станки.\n",
            "Смотрю,\n",
            "с ноги соборовых всему.\n",
            "Кровью потом.\n",
            "Сомненьем в строчку.\n",
            "Страна на бельсе.\n",
            "«Что ж, говорит,\n",
            "ни отец пором!\n",
            "Не старается ли?\n",
            "Вот в рядом высокий страны!\n",
            "Приходите ли как в положи!\n",
            "От весь пополнить за ногами лежат,\n",
            "стройки,\n",
            "как раз попросов на под ногами\n",
            "тех,\n",
            "но в коммунисты\n",
            "на корабы\n",
            "не подобные как с подымаемые красным масса.\n",
            "С который фантили солнце —\n",
            "на полицейский лаца.\n",
            "В подняться в столом\n",
            "провод стран\n",
            "и придется\n",
            "на заранут ворон.\n",
            "И вот в пусть и стран.\n",
            "Видно\n",
            "сегодня\n",
            "вставались\n",
            "теперь\n",
            "на хвост\n",
            "прикончить —\n",
            "как на первый разравления\n",
            "в полетки волин.\n",
            "И смерть\n",
            "пополничной массы.\n",
            "Вечер в кулака —\n",
            "всех\n",
            "с двигает торговой по конечного бедня,\n",
            "советской облига.\n",
            "Под носится года,\n",
            "сквозь в каждого столицы.\n",
            "Не дочина\n",
            "в глаза —\n",
            "одно последний попрос.\n",
            "Не старом над болото и калеча.\n",
            "Мальчик\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.73163366317749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = vocab_size*4\n",
        "# rnn_units = 500\n",
        "\n",
        "model_var_rnn_units = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim[4],\n",
        "    rnn_units=rnn_units[3])\n",
        "model_var_rnn_units.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "Hh1HCwWbs-17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# на 70 эпохах\n",
        "start = time.time()\n",
        "history_70 = model_var_rnn_units.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS_70, callbacks=[checkpoint_callback])\n",
        "end = time.time()\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEQtucIYtBfE",
        "outputId": "f322d3bf-cfb3-4943-b9ba-a285a9c980e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "102/102 [==============================] - 10s 59ms/step - loss: 3.1799 - val_loss: 2.7425\n",
            "Epoch 2/70\n",
            "102/102 [==============================] - 6s 46ms/step - loss: 2.6489 - val_loss: 2.5749\n",
            "Epoch 3/70\n",
            "102/102 [==============================] - 5s 39ms/step - loss: 2.5377 - val_loss: 2.4955\n",
            "Epoch 4/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 2.4722 - val_loss: 2.4383\n",
            "Epoch 5/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.4222 - val_loss: 2.3954\n",
            "Epoch 6/70\n",
            "102/102 [==============================] - 5s 43ms/step - loss: 2.3806 - val_loss: 2.3558\n",
            "Epoch 7/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.3443 - val_loss: 2.3220\n",
            "Epoch 8/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.3094 - val_loss: 2.2920\n",
            "Epoch 9/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 2.2792 - val_loss: 2.2626\n",
            "Epoch 10/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 2.2503 - val_loss: 2.2364\n",
            "Epoch 11/70\n",
            "102/102 [==============================] - 5s 40ms/step - loss: 2.2233 - val_loss: 2.2121\n",
            "Epoch 12/70\n",
            "102/102 [==============================] - 5s 36ms/step - loss: 2.1981 - val_loss: 2.1896\n",
            "Epoch 13/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.1758 - val_loss: 2.1689\n",
            "Epoch 14/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.1534 - val_loss: 2.1493\n",
            "Epoch 15/70\n",
            "102/102 [==============================] - 5s 36ms/step - loss: 2.1327 - val_loss: 2.1298\n",
            "Epoch 16/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.1146 - val_loss: 2.1158\n",
            "Epoch 17/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 2.0971 - val_loss: 2.1039\n",
            "Epoch 18/70\n",
            "102/102 [==============================] - 5s 36ms/step - loss: 2.0804 - val_loss: 2.0866\n",
            "Epoch 19/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 2.0646 - val_loss: 2.0769\n",
            "Epoch 20/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.0498 - val_loss: 2.0668\n",
            "Epoch 21/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.0339 - val_loss: 2.0561\n",
            "Epoch 22/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 2.0221 - val_loss: 2.0487\n",
            "Epoch 23/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 2.0084 - val_loss: 2.0364\n",
            "Epoch 24/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 1.9960 - val_loss: 2.0267\n",
            "Epoch 25/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.9836 - val_loss: 2.0222\n",
            "Epoch 26/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.9729 - val_loss: 2.0132\n",
            "Epoch 27/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.9615 - val_loss: 2.0046\n",
            "Epoch 28/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.9498 - val_loss: 1.9990\n",
            "Epoch 29/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.9398 - val_loss: 1.9953\n",
            "Epoch 30/70\n",
            "102/102 [==============================] - 5s 41ms/step - loss: 1.9288 - val_loss: 1.9863\n",
            "Epoch 31/70\n",
            "102/102 [==============================] - 6s 37ms/step - loss: 1.9185 - val_loss: 1.9806\n",
            "Epoch 32/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.9094 - val_loss: 1.9740\n",
            "Epoch 33/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.8993 - val_loss: 1.9699\n",
            "Epoch 34/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.8907 - val_loss: 1.9661\n",
            "Epoch 35/70\n",
            "102/102 [==============================] - 6s 37ms/step - loss: 1.8823 - val_loss: 1.9666\n",
            "Epoch 36/70\n",
            "102/102 [==============================] - 5s 39ms/step - loss: 1.8732 - val_loss: 1.9580\n",
            "Epoch 37/70\n",
            "102/102 [==============================] - 6s 44ms/step - loss: 1.8649 - val_loss: 1.9562\n",
            "Epoch 38/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.8562 - val_loss: 1.9534\n",
            "Epoch 39/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.8482 - val_loss: 1.9499\n",
            "Epoch 40/70\n",
            "102/102 [==============================] - 5s 39ms/step - loss: 1.8402 - val_loss: 1.9466\n",
            "Epoch 41/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.8324 - val_loss: 1.9425\n",
            "Epoch 42/70\n",
            "102/102 [==============================] - 5s 41ms/step - loss: 1.8245 - val_loss: 1.9435\n",
            "Epoch 43/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.8171 - val_loss: 1.9414\n",
            "Epoch 44/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.8091 - val_loss: 1.9390\n",
            "Epoch 45/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.8018 - val_loss: 1.9367\n",
            "Epoch 46/70\n",
            "102/102 [==============================] - 5s 39ms/step - loss: 1.7956 - val_loss: 1.9351\n",
            "Epoch 47/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 1.7878 - val_loss: 1.9323\n",
            "Epoch 48/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7808 - val_loss: 1.9347\n",
            "Epoch 49/70\n",
            "102/102 [==============================] - 5s 42ms/step - loss: 1.7739 - val_loss: 1.9316\n",
            "Epoch 50/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.7677 - val_loss: 1.9341\n",
            "Epoch 51/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7616 - val_loss: 1.9324\n",
            "Epoch 52/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7552 - val_loss: 1.9276\n",
            "Epoch 53/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.7476 - val_loss: 1.9328\n",
            "Epoch 54/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 1.7414 - val_loss: 1.9325\n",
            "Epoch 55/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7355 - val_loss: 1.9321\n",
            "Epoch 56/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.7299 - val_loss: 1.9312\n",
            "Epoch 57/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7236 - val_loss: 1.9302\n",
            "Epoch 58/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7163 - val_loss: 1.9303\n",
            "Epoch 59/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7117 - val_loss: 1.9333\n",
            "Epoch 60/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.7066 - val_loss: 1.9321\n",
            "Epoch 61/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.6998 - val_loss: 1.9339\n",
            "Epoch 62/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 1.6941 - val_loss: 1.9346\n",
            "Epoch 63/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.6881 - val_loss: 1.9329\n",
            "Epoch 64/70\n",
            "102/102 [==============================] - 5s 38ms/step - loss: 1.6818 - val_loss: 1.9383\n",
            "Epoch 65/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.6772 - val_loss: 1.9366\n",
            "Epoch 66/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.6720 - val_loss: 1.9381\n",
            "Epoch 67/70\n",
            "102/102 [==============================] - 6s 38ms/step - loss: 1.6662 - val_loss: 1.9405\n",
            "Epoch 68/70\n",
            "102/102 [==============================] - 5s 41ms/step - loss: 1.6602 - val_loss: 1.9429\n",
            "Epoch 69/70\n",
            "102/102 [==============================] - 5s 39ms/step - loss: 1.6553 - val_loss: 1.9447\n",
            "Epoch 70/70\n",
            "102/102 [==============================] - 5s 37ms/step - loss: 1.6500 - val_loss: 1.9438\n",
            "\n",
            "Run time: 407.541054725647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = model_var_rnn_units.evaluate(test_dataset)\n",
        "print('eval loss:',eval_loss)\n",
        "print('perplexity',np.exp(eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdxNJ5c_tEin",
        "outputId": "2ab8d6eb-efe3-4926-cab8-2b047277322a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 19ms/step - loss: 1.9626\n",
            "eval loss: 1.962598443031311\n",
            "perplexity 7.117798249863704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model_var_rnn_units, chars_from_ids, ids_from_chars)\n",
        "\n",
        "T = 0.5\n",
        "N = 1000\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['\\n'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(N):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states,temperature=T)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "result_text = result[0].numpy().decode('utf-8')\n",
        "print(result_text)\n",
        "print('_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSsxzz2atIxc",
        "outputId": "68cf9941-d743-4aef-b752-9a7c9a086720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Хоть выпускают в уши.\n",
            "В наши ль,\n",
            "не вывести\n",
            "распластанный восторг.\n",
            "\n",
            "</s>\n",
            "\n",
            "Мы\n",
            "было\n",
            "собственный и зам,\n",
            "в столе попробуй,\n",
            "с тоскою уловических души на двадцать.\n",
            "\n",
            "</s>\n",
            "\n",
            "Товарищ,\n",
            "платить по своем заколесами в город\n",
            "в работу «Кромоде»\n",
            "в комплекции\n",
            "не слышу —\n",
            "в звездно гроб,\n",
            "за столица\n",
            "в гробах волоса.\n",
            "Комсомольцы,\n",
            "вопрос не строй,\n",
            "в пальцы\n",
            "на борьбы\n",
            "расставились трестов.\n",
            "Виска —\n",
            "не исполнил своих леса.\n",
            "И просто —\n",
            "не верную перемери.\n",
            "А у без пересел и просто,\n",
            "а не вы,\n",
            "как под ногами\n",
            "стали\n",
            "на берегу в ремине.\n",
            "Он — солнце\n",
            "стоит руковичных облицей.\n",
            "В глазах\n",
            "распалял во все просторной,\n",
            "на ней\n",
            "носится\n",
            "старатель\n",
            "в делах вернель.\n",
            "\n",
            "\n",
            "Вот это —\n",
            "сегодня\n",
            "собака трудовой спинными моря,\n",
            "и не поповственных в столопал,\n",
            "но только\n",
            "слово не выставь, —\n",
            "на любовном работе\n",
            "в страже\n",
            "старее\n",
            "на столицей сплошном стих,\n",
            "в страшном небе\n",
            "не вылез ли\n",
            "с половина стали.\n",
            "Вся за домов\n",
            "не отдал бы не выйдет положенно.\n",
            "Копейки\n",
            "в общее пугуются в руки.\n",
            "На стене —\n",
            "мысль-рокот.\n",
            "Посудитель в мире —\n",
            "это значит,\n",
            "что в старою выружите\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.82841420173645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| epochs | embedding_dim | rnn_units | eval_loss | perplexity |\n",
        "|--------|---------------|-----------|-----------|------------|\n",
        "| 70     | vocab_size*4  | 10        | 2.552     | 12.836     |\n",
        "| 70     | vocab_size*4  | 100       | 2.146     | 8.547      |\n",
        "| 70     | vocab_size*4  | 300       | 1.974     | 7.203      |\n",
        "| 70     | vocab_size*4  | 500       | 1.962     | 7.118      |"
      ],
      "metadata": {
        "id": "2VDElbzXvFl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод:**"
      ],
      "metadata": {
        "id": "ufJu7TLxv0Op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "по результатам проведенных экспериментов можно сказать, что с ростом параметров embedding_dim и rnn_units показатели качества улучшаются, однако время обучения нейронной сети также возрастает. Сгенерированный текст становится более осмысленным, появляется все больше служебных символов, таких как точка, кавычки, запятые, тире и тд."
      ],
      "metadata": {
        "id": "gZrhlG8Ev3sI"
      }
    }
  ]
}